{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6fc9477",
   "metadata": {},
   "source": [
    "# ANN Model Training â€“ Telecom Customer Churn\n",
    "\n",
    "This notebook trains an Artificial Neural Network (ANN) to predict customer churn.\n",
    "\n",
    "Steps covered:\n",
    "- Load engineered dataset\n",
    "- Scale features (ANN requirement)\n",
    "- Build ANN architecture\n",
    "- Train the model\n",
    "- Evaluate performance using multiple metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dee2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7967e302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the data directory and file names \n",
    "\n",
    "data_dir = \"../data/processed/\"\n",
    "X_train_file = 'X_train.csv'\n",
    "y_train_file = 'y_train.csv'\n",
    "X_test_file = 'X_test.csv'\n",
    "y_test_file = 'y_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0171c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files \n",
    "import os\n",
    "\n",
    "X_train = pd.read_csv(os.path.join(data_dir, X_train_file)).values\n",
    "y_train = pd.read_csv(os.path.join(data_dir, y_train_file)).values\n",
    "\n",
    "X_test = pd.read_csv(os.path.join(data_dir, X_test_file)).values\n",
    "y_test = pd.read_csv(os.path.join(data_dir, y_test_file)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39505fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaling \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94773471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into PyTorch tensors \n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "if y_train_tensor.ndim == 1:\n",
    "  y_train_tensor = y_train_tensor.unsqueeze(1)  # adds a new dimension at index 1.\n",
    "\n",
    "# This condition will help to conver the tensor from 1D into 2D with shape (num_samples, 1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "if y_test_tensor.ndim == 1:\n",
    "  y_test_tensor = y_test_tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e639b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset and DataLoader\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "full_train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Split train into train + validation (e.g., 90% train, 10% val)\n",
    "val_size = int(0.1 * len(full_train_dataset))\n",
    "train_size = len(full_train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe055de",
   "metadata": {},
   "source": [
    "#### Build Artificial Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874272bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class churnANN(nn.Module):\n",
    "  def __init__(self, input_dim):\n",
    "    super(churnANN, self).__init__()\n",
    "    self.model = nn.Sequential(\n",
    "      nn.Linear(input_dim, 64),   # fully connnected layer \n",
    "      nn.ReLU(),                  # Rectified Linear Unit\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 1),\n",
    "      nn.Sigmoid()\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbba519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "model = churnANN(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dc7f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer \n",
    "\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0bd61a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/19 14:49:55 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/19 14:49:55 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
      "2025/12/19 14:49:55 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2025/12/19 14:49:56 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2025/12/19 14:49:56 INFO mlflow.tracking.fluent: Experiment with name 'Churn_ANN_Experiment' does not exist. Creating a new experiment.\n",
      "2025/12/19 14:50:00 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 43\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "models_dir = './models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "\n",
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(\"Churn_ANN_Experiment\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"input_dim\", input_dim)\n",
    "    mlflow.log_param(\"epochs\", num_epochs)\n",
    "    mlflow.log_param(\"batch_size\", 32)\n",
    "    mlflow.log_param(\"learning_rate\", 0.001)\n",
    "\n",
    "    # Training loop (same as before)\n",
    "    num_epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = criterion(val_outputs, y_val)\n",
    "                val_loss_total += val_loss.item() * X_val.size(0)\n",
    "        val_loss_avg = val_loss_total / len(val_loader.dataset)\n",
    "        \n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss_avg, step=epoch)\n",
    "\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            counter = 0\n",
    "            # Save model inside models directory\n",
    "            torch.save(model.state_dict(), os.path.join(models_dir, 'best_churn_model.pth'))\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                model.load_state_dict(torch.load(os.path.join(models_dir, 'best_churn_model.pth')))\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Log the final model to MLflow\n",
    "    mlflow.pytorch.log_model(model, \"churn_ann_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea0c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./models/epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38dc6658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9250\n",
      "Precision: 0.8052\n",
      "Recall:    0.6392\n",
      "F1 Score:  0.7126\n",
      "ROC-AUC:   0.8850\n",
      "Confusion Matrix:\n",
      "[[555  15]\n",
      " [ 35  62]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.97      0.96       570\n",
      "         1.0       0.81      0.64      0.71        97\n",
      "\n",
      "    accuracy                           0.93       667\n",
      "   macro avg       0.87      0.81      0.83       667\n",
      "weighted avg       0.92      0.93      0.92       667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor)\n",
    "    y_pred_class = (y_pred >= 0.5).float()\n",
    "\n",
    "# Convert to numpy for sklearn\n",
    "y_true = y_test_tensor.numpy()\n",
    "y_pred_class = y_pred_class.numpy()\n",
    "y_pred_prob = y_pred.numpy()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_true, y_pred_class)\n",
    "precision = precision_score(y_true, y_pred_class)\n",
    "recall = recall_score(y_true, y_pred_class)\n",
    "f1 = f1_score(y_true, y_pred_class)\n",
    "roc_auc = roc_auc_score(y_true, y_pred_prob)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {roc_auc:.4f}\")\n",
    "\n",
    "# Optional: Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_class))\n",
    "\n",
    "# Optional: Detailed report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_class))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
